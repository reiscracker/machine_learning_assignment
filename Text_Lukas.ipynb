{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Einleitung in den Datensatz\n",
    "    - Allgemein Challange\n",
    "    - Datenstruktur (Felder beschreiben)\n",
    "    - Zielsetzung\n",
    "\n",
    "- Visualisierung / Explorative untersuchung\n",
    "    - Wie viel von was...\n",
    "    - Cooccurrenten analyse\n",
    "\n",
    "- Predictig\n",
    "    - Preprocessing\n",
    "        - NaN Felder füllen\n",
    "        - Dummy Felder füllen\n",
    "    - LinearRegression\n",
    "    - RandomForest\n",
    "    - SupportVectorRegression\n",
    "    - Vergleich unserer Lösung\n",
    "- Fazit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Visualisierung / Explorative untersuchung\n",
    "\n",
    "## Kookkurrents-Analyse der Worte\n",
    "\n",
    "Die Artisten wurden von jedem Benutzer mit mehreren von 82 gegebenen Wörtern oder dem Tag \"None of these\" bewertet.\n",
    "Wenn man nun die Tags pro Artist als \"Satz\" interpretiert, kann man aus den Tags eine Kookkurrenzliste ableiten.\n",
    "\n",
    "Die Kookkurrenzliste is ein verschachtelter Hash, bzw. ein `dict`. Der `key` vom Eltern `dict` ist ein Wort. Der `key` vom Kinds `dict` ist das kookkurrierende Wort. Dieses wiederum hat als `value` die frequenz, wie oft es mit dem Eltern-Wort vorkommt.\n",
    "\n",
    "__Hier der Beginn der von uns erstellten Liste : __\n",
    "\n",
    "```json\n",
    "{\n",
    "    'Aggressive': {\n",
    "        'Annoying': 337,\n",
    "        'Approachable': 145,\n",
    "        'Arrogant': 1539,\n",
    "        'Authentic': 515,\n",
    "        ...\n",
    "     },\n",
    "     'Annoying': {\n",
    "        'Aggressive': 337,\n",
    "        'Approachable': 18,\n",
    "        'Arrogant': 372,\n",
    "        'Authentic': 50,\n",
    "        ...\n",
    "     },\n",
    "     ...\n",
    "}\n",
    "```\n",
    "\n",
    "### Berechnen der Signifikanz\n",
    "\n",
    "Nun kann durch die häufigkeit des gemeinsamem Auftretens die Signifikanz eines Wortes zu einem anderen berechnet werden.\n",
    "In einfachster form ist dies einfach deren gemeinsames auftreten wie schon erfasst.\n",
    "Nun gibt es jedoch noch komplexere formen der Signifikanzberechnungen wie der DICE-Koeffizient, die Loglikelihood und der Poisson-Mass. Diese sind alle gerichtete Signifikanzen. Das heißt Wort A ist zu wort B gleich signifikant wie Wort B zu Wort A. Das Poisson-Mass hat die besten Ergebnisse geliefert. Deswegen wird mit diesem weiter gearbeitet.\n",
    "\n",
    "![Image Alt](images/cooccurrence_tagclound.png)\n",
    "\n",
    "Aus dieser Grafik wurden die Tags \"None of These\" und \"Old\" entfernt. Dies da dies die zwei Tags mit der kleinsten signifikanz sind. Wenn diese dabei sind kann man die Unterschiede der anderen Tags kaum mehr sehen. Auch nicht gut zu sehen ist (desswegen rot erläutert) dass sich \"Talented\" ziemlich genau in der Mitte des Graphen befindet. Talentet ist, wie später errechnet wird, das signifikanteste Wort.\n",
    "\n",
    "![Image Alt](images/cooccurrence_non_of_these.png)\n",
    "\n",
    "Hier kann man sehr schön erkennen, dass \"Non of these\" nie mit einem anderen Wort vorkommt. Was ja auch sonst keinen Sinn machen würde.\n",
    "\n",
    "### Berechnung der Worte mit der höchsten Signifikanz mittels PageRank\n",
    "\n",
    "Wenn man die Kookkurrenten und deren Signifikanz analog zu Webseiten, welche durch Links auf einander Zeigen, interpretiert. Kann angenommen werden, dass auf die Kookkurrenzen eben so den PageRank-Algorithmus angewendet werden kann. Durch den PageRank-Algorithmus kann herausgefunden werden, welche Kookkurrenten, also Worte am meisten mit anderen Worten zusammen auftreten und so zusagen einen höheren Stellenwert besitzen.\n",
    "\n",
    "Um den PageRank zu berechnen verwenden wir die Python bibliothek \"networkx\". Diese erlaubt es einen Graphen auf zu bauen, und bietet die berechnung von PageRank sowie HITS an. Es wurde für jede Signifikanz berechnungsart (Poisson, Loglikelihood und Dice), einen Graphen erstellt und die PageRanks berechnet. Um ein möglichst ausgeglichenes Ergebnis zu erhalten wurden die Ergebnisse zusammengefügt und gemittelt.\n",
    "\n",
    "__ Die 30 signifikantesten Kookkurrenten __\n",
    "\n",
    "![Image Alt](images/most_significant_words_pagerank.png)\n",
    "\n",
    "Die Signifikantesten Terme kommen in der \"Tag Cloude\" eher in der mitte vor.\n",
    "\n",
    "Interessant ist hier, dass ein grossteil der signifikantesten Kookkurrenten, ebenfalls vom RandomForestRegressor als \"important Features\" angegeben wurde.\n",
    "\n",
    "### Weiterführende Gedanken zu der Kookkurrentenanalyse\n",
    "\n",
    "Nun könnte man diese Terme noch Clustern um neue Gruppierungen zu erhalten. Dann könnte man die 83 Suchterme auch kleinere Gruppen unterbrechen und so die Feature menge verkleinern, ohne dass ganze Terme gestrichen werden.\n",
    "In der Arbeit wurde experimentiert, dass nur die Wichtigsten worte verwendet werden. Dies hat zwar zum effekt dass die Vorhersagen mit weniger Features, trotzdem noch gut ist. Besser wurden es durch features reduktion jedoch nicht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predicting\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "### Analyse der Konsistenz in den CSV Dateien\n",
    "\n",
    "Wie zu sehen ist gibt es inkonsistenzen bei den User-Daten. In allen csv Dateien gibt es eine unterscheidliche Zahl von eineindeutigen Users. Wir werden beim Zusammenführen dieser Datein einen inner join verwenden damit nur noch Datensätze verwendet werden wo die User überall vorkommen. Leider können wir User nicht über einen Mittelwert auffüllen.\n",
    "\n",
    "    Unique Artists in train Frame 50\n",
    "    Unique Artists in words Frame 50\n",
    "    Unique Users in train Frame 49479\n",
    "    Unique Users in words Frame 50928\n",
    "    Unique Users in user Frame 48645\n",
    "    \n",
    "### Erstellen von Dummy-Werte/Spalten für nicht-nummerische Spalten\n",
    "\n",
    "Die Spalten `GENDER`, `REGION`, `WORKING` und `MUSIC` der Datei users.csv und die Spalten `HEARD_OF` und `OWN_ARTIST_MUSIC` der Datei words.cvs sind mit nicht-nummerischen Werten Gefüllt. Diese Werte müssen nun in nummerische Werte umgewandelt werden. Dafür werden sogenannte Dummy Werte erstellt. Dies kann mit der pandas funktion `pd.get_dummies` gemacht werden. Durch `pd.concat` werden die neu erstellten Spalten wieder mit dem pandas Dataframe zusammengeführt (konkatiniert).\n",
    "\n",
    "```python\n",
    "userFrameDummy = pd.concat([userFrame,\n",
    "                            pd.get_dummies(userFrame['GENDER'], prefix=\"sex\"),\n",
    "                            pd.get_dummies(userFrame['REGION'], prefix=\"region\"),\n",
    "                            pd.get_dummies(userFrame['WORKING'], prefix=\"work\"),\n",
    "                            pd.get_dummies(userFrame['MUSIC'], prefix=\"music\")], \n",
    "                           axis=1)\n",
    "\n",
    "wordsFrameDummy = pd.concat([wordsFrame,\n",
    "                             pd.get_dummies(wordsFrame['HEARD_OF'], prefix='heard'),\n",
    "                             pd.get_dummies(wordsFrame['OWN_ARTIST_MUSIC'], prefix='own')],\n",
    "                            axis=1)\n",
    "```\n",
    "\n",
    "Danach müssen die original Spalten entfernt werden da diese Werte ungültig, rsp nicht verarbeitet werden können.\n",
    "\n",
    "### Weitere Werteanpassungen\n",
    "\n",
    "__LIST_OWN und LIST_BACK__\n",
    "\n",
    "Die Felder _LIST_OWN_ und _LIST BACK_ beinhaltet eine Mischung zwischen nummerischen Nummern aber auch beschreibenden Nummern. Um alles nummerische Inhalte zu erhalten werden diese mit einer speziellen Funktion transformiert.\n",
    "\n",
    "```python\n",
    "def hourTransform(val):\n",
    "    if val in ['Less than an hour', '0', '0 Hours']:\n",
    "        return 0\n",
    "    if val in ['More than 16 hours', '16+ hours']:\n",
    "        return 18\n",
    "    for i in xrange(24):\n",
    "        if val in [str(i) + ' hour', str(i) + ' hours', str(i)]:\n",
    "            return i\n",
    "    return None\n",
    "```\n",
    "\n",
    "Diese Funktion wird dann auf die Spalte angewandt.\n",
    "\n",
    "```python\n",
    "userFrameDummy['LIST_OWN'] = userFrameDummy['LIST_OWN'].apply(hourTransform)\n",
    "```\n",
    "\n",
    "### Zusammenführen der Pandas Dataframes \n",
    "\n",
    "Nun werden die pandas Dataframes der Wörter, Tracks und Users zusammengeführt. Dabei wird explizit ein Inner Join verwedet, damit die inkonsistenten Daten wegfallen. Zu beachten ist, dass die User Spalte im train.csv den Namen __RESPID__ hat, in den anderen zwei Dateien jedoch __User__ heisst.\n",
    "\n",
    "```python\n",
    "X_all = pd.merge(trainFrame, userFrameDummy, how='inner', left_on='User', right_on='RESPID').drop('RESPID',1)\n",
    "X_all = pd.merge(X_all, wordsFrameDummy, how='inner', on=['Artist', 'User'])\n",
    "```\n",
    "\n",
    "__X_all__ ist nun ein pandas Dataframe welches alle Features und auch die Ragings beinahltet.\n",
    "\n",
    "### Trennen der Features mit dem Target\n",
    "\n",
    "Das zu erratende Feld (target) ist das `Rating`. Deswegen wird nun diese Spalte von X_all entfernt und in y_all hinzugefügt.\n",
    "\n",
    "```python\n",
    "y_all = X_all['Rating']\n",
    "X_all = X_all.drop('Rating',1)\n",
    "```\n",
    "### Trennen der Trainingsdaten in Train -und Testdaten\n",
    "\n",
    "Das Training sollte nicht auf den Testdaten passieren. Damit wir sicher nicht eine Abhängigkeit schaffen werden die Test und Trainingsdaten getrennt bevor die NaN Werte aufgefüllt werden. Dafür verwenden wir die von sklearn.cross_validation gestellte funktion `train_test_split`. Dabei werden 20% der Daten in Testdaten getrennt. Die Daten werden zufällig heraus gepickt. Es wurde diese Zufälligkeit gewählt um zu verhindert dass z.B bewertungen von gesamten Benutzern fehlen.\n",
    "\n",
    "```python\n",
    "from sklearn import cross_validation\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "    X_all, y_all, test_size=0.20, random_state=2)\n",
    "```\n",
    "## Training\n",
    "\n",
    "### Base LinearRegression\n",
    "\n",
    "Für das \"base model\" wurde die Lineare Regression gewählt. Dabei wurde so vorgegangen:\n",
    "\n",
    "- Die Trainingsdaten werden nochmals durch Crossvalidation in 3 Teile gesplittet.\n",
    "- Es wird sklearn.pipeline verwendet die Reihenfolge zu \"automatisieren\".\n",
    "- Durch preprocessing.Imputer werden die NaN felder mit dem Mittelwert gefüllt.\n",
    "- Durch proprocessing.StandardScalar werden die features scaliert.\n",
    "- der cross_validation.cross_val_score berechnet den mean_squared_error der corssvalidation.\n",
    "\n",
    "Es ist uns aufgefallen, dass die Methode `cross_val_score` einen negativen mean_squared_error zurück gibt. Näheres kann auf Githup nachgelesen werden. Wir konnten das Problem lösen indem wir einfach das Resultat wieder mit -1 multiplizierten.\n",
    "https://github.com/scikit-learn/scikit-learn/issues/2439.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "cv = cross_validation.ShuffleSplit(X_train.shape[0], n_iter=3,\n",
    "    test_size=0.3, random_state=0)\n",
    "\n",
    "mean_preprocessor = preprocessing.Imputer(strategy=\"mean\", axis=0)\n",
    "scaler_preprocessor = preprocessing.StandardScaler()\n",
    "clf = make_pipeline(mean_preprocessor, scaler_preprocessor, model)\n",
    "scores = cross_validation.cross_val_score(clf, X_train, y_train, scoring=\"mean_squared_error\", cv=cv)\n",
    "(scores.mean() * -1) **0.5\n",
    "\n",
    "# >> durchschnittliche rmse => 16.15075547663826\n",
    "```\n",
    "\n",
    "__ Auf Testdaten anwenden __\n",
    "\n",
    "- Zuerst werden die Mittelwerte aufgefüllt und zwar separat für die Testdaten und die Trainingsdaten.\n",
    "- Danach wird das model mit den Trainingsdaten trainiert. Nun werden alle Trainingsdaten verwendet.\n",
    "- Zuletzt werden die Testdaten vorhergesagt und den mean_squared_error davon berechnet.\n",
    "\n",
    "```python\n",
    "X_test_mean = mean_preprocessor.fit_transform(X_test)\n",
    "X_train_mean = mean_preprocessor.fit_transform(X_train)\n",
    "model = model.fit(X_train_mean, y_train)\n",
    "mse = metrics.mean_squared_error(model.predict(X_test_mean), y_test)\n",
    "rmse = mse**0.5\n",
    "\n",
    "# >> rmse => 16.220828137494721\n",
    "```\n",
    "Mit diesem Model konnten wir auf Kaggle einen guten Mittelfeld Platz ergattern.\n",
    "\n",
    "### RidgeRegressor\n",
    "\n",
    "Der RidgeRegressor ist eine Erweiterung des LinearRegressor durch regularisierung. Nun ist es möglich einen Parameter `alpha` mit zu geben. Je grösser dieser Wert gewählt wird, desto mehr wird versucht der polynomen Funktion (da wir mehrere Features haben) die Dimensionen zu verkleiner. Es ist also eine Methode um Overfitting zu verhindern.\n",
    "\n",
    "![Image Alt](images/ridge_diff_to_linear_per_alpha.png)\n",
    "\n",
    "Wie man an der Grafik sehen kann, ändern sich die Werte pro alpha minimalst. Und zusätzlich sind sie gegenübergestellt des rsme von __16.15075547663826__ der einfachen LinearenRegression alle schlechter.\n",
    "\n",
    "### LassoRegressor\n",
    "\n",
    "Eine weitere Erweiterung der LinearenRegression ist der LassoRegressor. Dieser versucht ebenfalls wie der RidgeRegressor eine gewisse regualrisation zu erreichen. Genauer versucht der LassoRegressor wichtige features zu finden und nur diese zu verwenden. Der LassoRegressor hat jedoch auch nur schlechtere ergebnisse geliefert als der normale LineareRegressor. \n",
    "\n",
    "Es wurden alphas zwischen 0.001 und 0.5 gewählt. Dabei hat die der rsme nicht verändert bei __16.150947926890396__ gehalten.\n",
    "\n",
    "### RandomForest\n",
    "\n",
    "Der RandomForst Klassifikator/Regressor generiert mehrere Entscheidungsbäume. Wenn nun eine neue vorhersage gemacht wird, wird diese von allen Entscheidungsbäumen gefällt und zusammengezählt. Jeder Baum im Wald hat somit mitentscheidungsrecht. Die Antwort für welche die meisten Bäume wahren gewinnt.\n",
    "Der RandomForest kann nicht \"Overfitten\" und ist ziemlich schnell, auch wenn mehrere Bäume generiert werden.\n",
    "Durch dass der RandomForest die Bäume durch zufällige features generiert, kann er auch berechnen welche Fetures relevant sind und welche nicht. Daher kann er auch sehr gut zur Feature selektion genutzt werden.\n",
    "\n",
    "Der RandomForest mit 100 Bäumen erreicht eine rmse von __14.568395227441838__ welche deutlich oberhalb der LinearRegression ligt und den 23. Rang bei Kaggle ist.\n",
    "\n",
    "__ Wichtige Features vom Random Forest __\n",
    "\n",
    "![Image Alt](images/important_features_random_forest_100.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Wie in der Grafik zu sehen ist sind die 10 Wichtigsten Features AGE, Q11, User, Boring, Beautiful, Catchy, Track, LIKE_ARTIST und Talented. Vergleicht man diese Features, mit der anayse der signifikanten Kookkurrenten, sieht man dass alle wichtigen Features - so fern es Wörter sind - auch wichtige Kookkurrenten sind.\n",
    "\n",
    "![alt test](images/all_features_improtantes.png)\n",
    "\n",
    "Werden alle wichtigen Features ausgegeben kann sehr gut gesehen gelesen werden, dass nur das erste drittel der Features wichtig ist, die anderen streben sehr gegen 0.\n",
    "\n",
    "### LinearRegression mit wichtigen Features\n",
    "\n",
    "Nun, da die wichtigsten Features durch RandomForest heausgefunden wurde, ist von Interesse wie sich die normale LineareRegression verhält, wenn diese nur mit den wichtigsten Features gemacht wird.\n",
    "\n",
    "Um zu testen ob es sich um Overfitting handelt, haben wir nun mal die LineareRegression anhand nur den wichtigsten Features durchgeführt.\n",
    "\n",
    "![alt text](images/rsme_important_features.png)\n",
    "\n",
    "In dieser Grafik kann sehr schön abgelesen werden dass die wichtigsten Features vom RandomForest tatsächlich viel wichtiger sind als die Schwachen. Auch sieht man eine schöne Parallele zur letzen Grafik, wo die Wichtigkeit der Features ausgegeben wird. Dies ist dadurch sichtbar, dass am anfang der rsme sehr steil nach unten geht. Jedoch wird das Resultat pro feature immer besser! Mit 100 Features wird immer noch ein gutes Resultat erhalten. Es könnten also 50 Features gespart werden.\n",
    "\n",
    "### LinearRegression mit PCA\n",
    "\n",
    "Die Anzeichen der bisherigen Analysen deuten darauf hin, dass unsere Daten eher an underfitting als overfitting leiden. Als letzte Prüfung wird eine PCA (Principal Component Analysis) auf die Daten und der LinearenRegression angewandt. PCA ist ein Ferfahren mit welchem die Dimensionen Reduziert werden durch dass unwichtige Informationen verworfen werden. Dabei werden jedoch nicht ganze Features entfehrnt, viel mehr werden die vorhandenen Features in neue Features Transferiert und dabei die unwichtigen Elemente der einzelnen Features entfehrnt.\n",
    "\n",
    "![alt text](images/pca_linear_regression.png)\n",
    "\n",
    "Auf dem Bild wird gezeit, wie der rsme mit steigender Anzahl Features immer kleine und somit besser wird. Interessanter weise ist der pca mit 149 Features, also einem Feature weniger als im Orginal der beste Wert von __16.150403__ besitzt und somit sogar minimal besser ist als ohne PCA. In der Grafik sind die Unterschiede im hundertstel bereich sichtbar, so stark wie es scheint sind die unterschiede nicht.\n",
    "\n",
    "Nach dieser Erkentniss, sind wir davon überzeugt, dass das Resultat nur verbessert werden kann, wenn noch mehr features durch _Feature engeneering_ generiert werden. Nicht aber durch Aussortierung schlechter Features.\n",
    "\n",
    "### Support Vector Regression\n",
    "\n",
    "Auch haben wir versucht die Bewertungen per Support Vector regression vorher zu sagen. Dieser dauerte jedoch auf einer Machine 5 Stunden. Somit haben wir ihn nur einmal mit Standardwerten laufen lassen. Dabei ist ein eher ernüchternder rsme von __22.237846871998496__ herausgekommen. Evt. könnte man per GridSearch noch bessere hyperparameter finden. Dafür haben wir jedoch zu wenig rechenpower ;)\n",
    "\n",
    "### Feature engeneering\n",
    "\n",
    "__ Altersgruppen __\n",
    "\n",
    "Um auf weitere Features zur Feature Engeneering zu kommen hat sich als erstes das Alter angeboten. Dabei habe wurden die Alter in zehn Jahres Gruppen aufgeteilt. Somit wurden 8 neue Features generiert.\n",
    "\n",
    "LineareRegression mit Gruppierung: rmse = __16.149953208751128__\n",
    "RandomForest mit Gruppierung: rmse = __14.562964016928039__\n",
    "\n",
    "Somit wurde eine minimale Verbesserung unter LinearRegression und auch unter RandomForest erreicht.\n",
    "\n",
    "## Test\n",
    "\n",
    "Nun werden die Modelle mit den Testdaten getestet. Dabei müssen die Features der Testdaten auch angepasst werden. Besonders müssen ebenfalls die NaN Felder Gefüllt werden und auch die AGE Gruppierung gemacht werden, da die Testdaten die gleiche anzahl Features haben müssen wie die Trainingsdaten.\n",
    "\n",
    "- LinearRegression mit AGE Gruppierung: rmse = __16.219554709616276__\n",
    "- RandomForest mit AGE Gruppierung: rmse = __14.376954514210322__\n",
    "\n",
    "Hier ist interessant zu sehen dass der RandomForest auf die eigenen Testdaten sogar ein besseres Ergebnis erziehlt als mit crossvalidierung der Trainingsdaten. Auf Kaggle ist dies der 23. Platz.\n",
    "\n",
    "## Ausblick\n",
    "\n",
    "### Weiteres Feature engeneering\n",
    "\n",
    "Da wir nun herausgefunden haben, dass sich duch Feature-Engeneering, also Erweiterung der Features das Resulat verbessern lässt, würde sich folgendes Szenario anbieten.\n",
    "\n",
    "1. Analysieren der besten Fragen\n",
    "    - Kann man die User anhand der Fragen in weitere Gruppen einteilen?\n",
    "2. Clustering der Kookkurrenten\n",
    "    - Kann man die Wörter noch mals Gruppieren und so neue Features erstellen?\n",
    "    - Erweitern der Kookkurrenten durch Synonyme.\n",
    "\n",
    "### Collaborative Filtering\n",
    "\n",
    "Auch gibt es noch grundsätzlich andere Verfahren zur Berechnung von Bewertungen. Diese unter dem Namen \"Collaborative Filtering\" funktionierenden Systeme versuchen neuer Ratings aus den Ratings ähnlicher Benutzer und Tracks zu schliessen. Aus Zeitgründen wurden diese Verfahren von uns nicht getestet.\n",
    "\n",
    "Collaboratives Filtering könnte so aussehen:\n",
    "\n",
    "1. Es muss ein Ähnlichkeitsmaß der Benutzer, Artists und Tracks gefunden werden.\n",
    "2. Die Bewertung geht nun folgendermassen:\n",
    "    - Es werden die Ähnlichsten Benutzer zum zu bewertenden Benutzer gesucht.\n",
    "    - Es werden die Tracks der ähnlichen Benutzer welche auch ähnlich oder gleich dem neu zu bewertenden Track gesucht.\n",
    "    - Von diesen Tracks wird der Mittelwert der Ratings genommen.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
