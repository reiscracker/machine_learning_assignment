{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Einleitung in den Datensatz\n",
    "    - Allgemein Challange\n",
    "    - Datenstruktur (Felder beschreiben)\n",
    "    - Zielsetzung\n",
    "\n",
    "- Visualisierung / Explorative untersuchung\n",
    "    - Wie viel von was...\n",
    "    - Cooccurrenten analyse\n",
    "\n",
    "- Predictig\n",
    "    - Preprocessing\n",
    "        - NaN Felder füllen\n",
    "        - Dummy Felder füllen\n",
    "    - LinearRegression\n",
    "    - RandomForest\n",
    "    - SupportVectorRegression\n",
    "    - Vergleich unserer Lösung\n",
    "- Fazit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Visualisierung / Explorative untersuchung\n",
    "\n",
    "## Kookkurrents-Analyse der Worte\n",
    "\n",
    "Die Artisten wurden von jedem Benutzer mit mehreren von 82 gegebenen Wörtern oder dem Tag \"None of these\" bewertet.\n",
    "Wenn man nun die Tags pro Artist als \"Satz\" interpretiert, kann man aus den Tags eine Kookkurrenzliste ableiten.\n",
    "\n",
    "Die Kookkurrenzliste is ein verschachtelter Hash, bzw. ein `dict`. Der `key` vom Eltern `dict` ist ein Wort. Der `key` vom Kinds `dict` ist das kookkurrierende Wort. Dieses wiederum hat als `value` die frequenz, wie oft es mit dem Eltern-Wort vorkommt.\n",
    "\n",
    "__Hier der Beginn der von uns erstellten Liste : __\n",
    "\n",
    "    {\n",
    "        'Aggressive': {\n",
    "            'Annoying': 337,\n",
    "            'Approachable': 145,\n",
    "            'Arrogant': 1539,\n",
    "            'Authentic': 515,\n",
    "            ...\n",
    "         },\n",
    "         'Annoying': {\n",
    "            'Aggressive': 337,\n",
    "            'Approachable': 18,\n",
    "            'Arrogant': 372,\n",
    "            'Authentic': 50,\n",
    "            ...\n",
    "         },\n",
    "         ...\n",
    "    }\n",
    "\n",
    "### Berechnen der Signifikanz\n",
    "\n",
    "Nun kann durch die häufigkeit des gemeinsamem Auftretens die Signifikanz eines Wortes zu einem anderen berechnet werden.\n",
    "In einfachster form ist dies einfach deren gemeinsames auftreten wie schon erfasst.\n",
    "Nun gibt es jedoch noch komplexere formen der Signifikanzberechnungen wie der DICE-Koeffizient, die Loglikelihood und der Poisson-Mass. Diese sind alle gerichtete Signifikanzen. Das heißt Wort A ist zu wort B gleich signifikant wie Wort B zu Wort A. Das Poisson-Mass hat die besten Ergebnisse geliefert. Deswegen wird mit diesem weiter gearbeitet.\n",
    "\n",
    "![Image Alt](images/cooccurrence_tagclound.png)\n",
    "\n",
    "Aus dieser Grafik wurden die Tags \"None of These\" und \"Old\" entfernt. Dies da dies die zwei Tags mit der kleinsten signifikanz sind. Wenn diese dabei sind kann man die Unterschiede der anderen Tags kaum mehr sehen. Auch nicht gut zu sehen ist (desswegen rot erläutert) dass sich \"Talented\" ziemlich genau in der Mitte des Graphen befindet. Talentet ist, wie später errechnet wird, das signifikanteste Wort.\n",
    "\n",
    "![Image Alt](images/cooccurrence_non_of_these.png)\n",
    "\n",
    "Hier kann man sehr schön erkennen, dass \"Non of these\" nie mit einem anderen Wort vorkommt. Was ja auch sonst keinen Sinn machen würde.\n",
    "\n",
    "### Berechnung der Worte mit der höchsten Signifikanz mittels PageRank\n",
    "\n",
    "Wenn man die Kookkurrenten und deren Signifikanz analog zu Webseiten, welche durch Links auf einander Zeigen, interpretiert. Kann angenommen werden, dass auf die Kookkurrenzen eben so den PageRank-Algorithmus angewendet werden kann. Durch den PageRank-Algorithmus kann herausgefunden werden, welche Kookkurrenten, also Worte am meisten mit anderen Worten zusammen auftreten und so zusagen einen höheren Stellenwert besitzen.\n",
    "\n",
    "Um den PageRank zu berechnen verwenden wir die Python bibliothek \"networkx\". Diese erlaubt es einen Graphen auf zu bauen, und bietet die berechnung von PageRank sowie HITS an. Es wurde für jede Signifikanz berechnungsart (Poisson, Loglikelihood und Dice), einen Graphen erstellt und die PageRanks berechnet. Um ein möglichst ausgeglichenes Ergebnis zu erhalten wurden die Ergebnisse zusammengefügt und gemittelt.\n",
    "\n",
    "__ Die 30 signifikantesten Kookkurrenten __\n",
    "\n",
    "![Image Alt](images/most_significant_words_pagerank.png)\n",
    "\n",
    "Die Signifikantesten Terme kommen in der \"Tag Cloude\" eher in der mitte vor.\n",
    "\n",
    "Interessant ist hier, dass ein grossteil der signifikantesten Kookkurrenten, ebenfalls vom RandomForestRegressor als \"important Features\" angegeben wurde.\n",
    "\n",
    "### Weiterführende Gedanken zu der Kookkurrentenanalyse\n",
    "\n",
    "Nun könnte man diese Terme noch Clustern um neue Gruppierungen zu erhalten. Dann könnte man die 83 Suchterme auch kleinere Gruppen unterbrechen und so die Feature menge verkleinern, ohne dass ganze Terme gestrichen werden.\n",
    "In der Arbeit wurde experimentiert, dass nur die Wichtigsten worte verwendet werden. Dies hat zwar zum effekt dass die Vorhersagen mit weniger Features, trotzdem noch gut ist. Besser wurden es durch features reduktion jedoch nicht."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predicting\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "### Analyse der Konsistenz in den CSV Dateien\n",
    "\n",
    "Wie zu sehen ist gibt es inkonsistenzen bei den User-Daten. In allen csv Dateien gibt es eine unterscheidliche Zahl von eineindeutigen Users. Wir werden beim Zusammenführen dieser Datein einen inner join verwenden damit nur noch Datensätze verwendet werden wo die User überall vorkommen. Leider können wir User nicht über einen Mittelwert auffüllen.\n",
    "\n",
    "    Unique Artists in train Frame 50\n",
    "    Unique Artists in words Frame 50\n",
    "    Unique Users in train Frame 49479\n",
    "    Unique Users in words Frame 50928\n",
    "    Unique Users in user Frame 48645\n",
    "    \n",
    "### Erstellen von Dummy-Werte/Spalten für nicht-nummerische Spalten\n",
    "\n",
    "Die Spalten `GENDER`, `REGION`, `WORKING` und `MUSIC` der Datei users.csv und die Spalten `HEARD_OF` und `OWN_ARTIST_MUSIC` der Datei words.cvs sind mit nicht-nummerischen Werten Gefüllt. Diese Werte müssen nun in nummerische Werte umgewandelt werden. Dafür werden sogenannte Dummy Werte erstellt. Dies kann mit der pandas funktion `pd.get_dummies` gemacht werden. Durch `pd.concat` werden die neu erstellten Spalten wieder mit dem pandas Dataframe zusammengeführt (konkatiniert).\n",
    "\n",
    "```python\n",
    "userFrameDummy = pd.concat([userFrame,\n",
    "                            pd.get_dummies(userFrame['GENDER'], prefix=\"sex\"),\n",
    "                            pd.get_dummies(userFrame['REGION'], prefix=\"region\"),\n",
    "                            pd.get_dummies(userFrame['WORKING'], prefix=\"work\"),\n",
    "                            pd.get_dummies(userFrame['MUSIC'], prefix=\"music\")], \n",
    "                           axis=1)\n",
    "\n",
    "wordsFrameDummy = pd.concat([wordsFrame,\n",
    "                             pd.get_dummies(wordsFrame['HEARD_OF'], prefix='heard'),\n",
    "                             pd.get_dummies(wordsFrame['OWN_ARTIST_MUSIC'], prefix='own')],\n",
    "                            axis=1)\n",
    "```\n",
    "\n",
    "Danach müssen die original Spalten entfernt werden da diese Werte ungültig, rsp nicht verarbeitet werden können.\n",
    "\n",
    "### Weitere Werteanpassungen\n",
    "\n",
    "__LIST_OWN und LIST_BACK__\n",
    "\n",
    "Die Felder _LIST_OWN_ und _LIST BACK_ beinhaltet eine Mischung zwischen nummerischen Nummern aber auch beschreibenden Nummern. Um alles nummerische Inhalte zu erhalten werden diese mit einer speziellen Funktion transformiert.\n",
    "\n",
    "```python\n",
    "def hourTransform(val):\n",
    "    if val in ['Less than an hour', '0', '0 Hours']:\n",
    "        return 0\n",
    "    if val in ['More than 16 hours', '16+ hours']:\n",
    "        return 18\n",
    "    for i in xrange(24):\n",
    "        if val in [str(i) + ' hour', str(i) + ' hours', str(i)]:\n",
    "            return i\n",
    "    return None\n",
    "```\n",
    "\n",
    "Diese Funktion wird dann auf die Spalte angewandt.\n",
    "\n",
    "```python\n",
    "userFrameDummy['LIST_OWN'] = userFrameDummy['LIST_OWN'].apply(hourTransform)\n",
    "```\n",
    "\n",
    "### Zusammenführen der Pandas Dataframes \n",
    "\n",
    "Nun werden die pandas Dataframes der Wörter, Tracks und Users zusammengeführt. Dabei wird explizit ein Inner Join verwedet, damit die inkonsistenten Daten wegfallen.\n",
    "\n",
    "```python\n",
    "X_all = pd.merge(trainFrame, userFrameDummy, how='inner', left_on='User', right_on='RESPID').drop('RESPID',1)\n",
    "X_all = pd.merge(X_all, wordsFrameDummy, how='inner', on=['Artist', 'User'])\n",
    "```\n",
    "\n",
    "__X_all__ ist nun ein pandas Dataframe welches alle Features und auch die Ragings beinahltet.\n",
    "\n",
    "### Trennen der Features mit dem Target\n",
    "\n",
    "Das zu erratende Feld (target) ist das `Rating`. Deswegen wird nun diese Spalte von X_all entfernt und in y_all hinzugefügt.\n",
    "\n",
    "```python\n",
    "y_all = X_all['Rating']\n",
    "X_all = X_all.drop('Rating',1)\n",
    "```\n",
    "### Trennen der Trainingsdaten in Train -und Testdaten\n",
    "\n",
    "Das Training sollte nicht auf den Testdaten passieren. Damit wir sicher nicht eine Abhängigkeit schaffen werden die Test und Trainingsdaten getrennt bevor die NaN Werte aufgefüllt werden. Dafür verwenden wir die von sklearn.cross_validation gestellte funktion `train_test_split`. Dabei werden 20% der Daten in Testdaten getrennt. Die Daten werden zufällig heraus gepickt. Es wurde diese Zufälligkeit gewählt um zu verhindert dass z.B bewertungen von gesamten Benutzern fehlen.\n",
    "\n",
    "```python\n",
    "from sklearn import cross_validation\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "    X_all, y_all, test_size=0.20, random_state=2)\n",
    "```\n",
    "## Training\n",
    "\n",
    "### Base LinearRegression\n",
    "\n",
    "Für das \"base model\" wurde die Lineare Regression gewählt. Dabei wurde so vorgegangen:\n",
    "\n",
    "- Die Trainingsdaten werden nochmals durch Crossvalidation in 3 Teile gesplittet.\n",
    "- Es wird sklearn.pipeline verwendet die Reihenfolge zu \"automatisieren\".\n",
    "- Durch preprocessing.Imputer werden die NaN felder mit dem Mittelwert gefüllt.\n",
    "- Durch proprocessing.StandardScalar werden die features scaliert.\n",
    "- der cross_validation.cross_val_score berechnet den mean_squared_error der corssvalidation.\n",
    "\n",
    "Es ist uns aufgefallen, dass die Methode `cross_val_score` einen negativen mean_squared_error zurück gibt. Näheres kann auf Githup nachgelesen werden. Wir konnten das Problem lösen indem wir einfach das Resultat wieder mit -1 multiplizierten.\n",
    "https://github.com/scikit-learn/scikit-learn/issues/2439.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "cv = cross_validation.ShuffleSplit(X_train.shape[0], n_iter=3,\n",
    "    test_size=0.3, random_state=0)\n",
    "\n",
    "mean_preprocessor = preprocessing.Imputer(strategy=\"mean\", axis=0)\n",
    "scaler_preprocessor = preprocessing.StandardScaler()\n",
    "clf = make_pipeline(mean_preprocessor, scaler_preprocessor, model)\n",
    "scores = cross_validation.cross_val_score(clf, X_train, y_train, scoring=\"mean_squared_error\", cv=cv)\n",
    "(scores.mean() * -1) **0.5\n",
    "\n",
    "# >> durchschnittliche rmse => 16.15075547663826\n",
    "```\n",
    "\n",
    "__ Auf Testdaten anwenden __\n",
    "\n",
    "- Zuerst werden die mittelwerte aufgefüllt und zwar separat für die Testdaten und die Trainingsdaten.\n",
    "- Danach wird das model mit den Trainingsdaten trainiert. Nun werden alle Trainingsdaten verwendet.\n",
    "- Zuletzt werden die Testdaten vorhergesagt und den mean_squared_error davon berechnet.\n",
    "\n",
    "```python\n",
    "X_test_mean = mean_preprocessor.fit_transform(X_test)\n",
    "X_train_mean = mean_preprocessor.fit_transform(X_train)\n",
    "model = model.fit(X_train_mean, y_train)\n",
    "mse = metrics.mean_squared_error(model.predict(X_test_mean), y_test)\n",
    "rmse = mse**0.5\n",
    "\n",
    "# >> rmse => 16.220828137494721\n",
    "```\n",
    "Mit diesem Model konnten wir auf Kaggle einen guten Mittelfeld Platz ergattern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
